import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, VideoProcessorBase, RTCConfiguration
import cv2
import mediapipe as mp
import numpy as np
import joblib
import av
from collections import deque
import math

# =========================================================
# 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# =========================================================
MODEL_PATH = 'model.joblib'  # ëª¨ë¸ íŒŒì¼ ì´ë¦„ í™•ì¸!
SEQ_LENGTH = 40

# ì§€ë„ ë° í™”ë©´ ì„¤ì •
MAP_WIDTH = 500
CAM_WIDTH = 500
HEIGHT = 700
TOTAL_WIDTH = MAP_WIDTH + CAM_WIDTH

# ìƒ‰ìƒ (OpenCVëŠ” BGR ìˆœì„œ)
BG_COLOR = (40, 40, 40)
ROOM_COLOR = (200, 200, 200)
ROBOT_COLOR = (50, 50, 255) # ë¹¨ê°• (BGR)
TEXT_COLOR = (0, 0, 0)

# ë§µ ìœ„ì¹˜ (ì¤‘ì‹¬ x, ì¤‘ì‹¬ y, ë„ˆë¹„, ë†’ì´) - Map ì˜ì—­ ê¸°ì¤€
ROOMS = {
    'toilet':   (250, 100, 200, 80),
    'room2':    (250, 220, 200, 80),
    'room1':    (250, 340, 200, 80),
    'elevator': (250, 460, 200, 80),
    'home':     (250, 600, 80, 50)
}

# =========================================================
# 2. ëª¨ë¸ ë¡œë“œ (ìºì‹± ì‚¬ìš©)
# =========================================================
@st.cache_resource
def load_model():
    try:
        return joblib.load(MODEL_PATH)
    except Exception as e:
        return None

# =========================================================
# 3. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
# =========================================================
def extract_xyz(hand_lms):
    if hand_lms is None: return [0.0] * 63
    out = []
    for lm in hand_lms.landmark:
        out.extend([lm.x, lm.y, lm.z])
    return out

# =========================================================
# 4. ì˜ìƒ ì²˜ë¦¬ê¸° í´ë˜ìŠ¤ (í•µì‹¬ ë¡œì§)
# =========================================================
class SignLanguageProcessor(VideoProcessorBase):
    def __init__(self):
        # ëª¨ë¸ ë° ë„êµ¬ ì´ˆê¸°í™”
        self.model = load_model()
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            max_num_hands=2, 
            min_detection_confidence=0.5, 
            min_tracking_confidence=0.5
        )
        
        # ë°ì´í„° ë²„í¼
        self.seq_buffer = deque(maxlen=SEQ_LENGTH)
        
        # ë¡œë´‡ ìƒíƒœ
        self.rx, self.ry = ROOMS['home'][0], ROOMS['home'][1]
        self.tx, self.ty = self.rx, self.ry
        self.speed = 4
        self.status = "Ready"
        self.last_action = "None"
        self.confidence = 0.0

    def update_robot(self):
        # ë¡œë´‡ ì´ë™ ë¡œì§
        dx = self.tx - self.rx
        dy = self.ty - self.ry
        dist = math.hypot(dx, dy)

        if dist > self.speed:
            self.rx += (dx / dist) * self.speed
            self.ry += (dy / dist) * self.speed
        else:
            self.rx = self.tx
            self.ry = self.ty
            if "Moving" in self.status or "Returning" in self.status:
                self.status = "Arrived"

    def draw_map(self, canvas):
        # 1. ë°°ê²½ (ì™¼ìª½ ì ˆë°˜)
        cv2.rectangle(canvas, (0, 0), (MAP_WIDTH, HEIGHT), BG_COLOR, -1)
        
        # 2. ë³µë„ ë¼ì¸
        cv2.line(canvas, (250, 100), (250, 600), (100, 100, 100), 10)

        # 3. ë°© ê·¸ë¦¬ê¸°
        for name, (cx, cy, w, h) in ROOMS.items():
            color = ROOM_COLOR
            if name == 'home': color = (255, 100, 100) # Blue-ish (BGR)
            elif name == 'elevator': color = (100, 255, 255) # Yellow
            elif name == 'toilet': color = (255, 255, 100) # Cyan

            # ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
            tl = (cx - w//2, cy - h//2)
            br = (cx + w//2, cy + h//2)
            cv2.rectangle(canvas, tl, br, color, -1)
            cv2.rectangle(canvas, tl, br, (255, 255, 255), 2)
            
            # í…ìŠ¤íŠ¸ (OpenCV í°íŠ¸)
            cv2.putText(canvas, name.upper(), (cx - 40, cy + 5), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, TEXT_COLOR, 2)

        # 4. ë¡œë´‡ ê·¸ë¦¬ê¸°
        cv2.circle(canvas, (int(self.rx), int(self.ry)), 15, ROBOT_COLOR, -1)
        cv2.circle(canvas, (int(self.rx), int(self.ry)), 15, (255,255,255), 2)

    def recv(self, frame):
        # 1. ì›¹ìº  í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (av -> numpy)
        img = frame.to_ndarray(format="bgr24")
        
        # ê±°ìš¸ ëª¨ë“œ í•´ì œ (í•™ìŠµ ë°ì´í„°ì™€ ë§ì¶¤) - í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
        # img = cv2.flip(img, 1)

        # 2. ì „ì²´ ìº”ë²„ìŠ¤ ë§Œë“¤ê¸° (ì™¼ìª½: ì§€ë„, ì˜¤ë¥¸ìª½: ì¹´ë©”ë¼)
        canvas = np.zeros((HEIGHT, TOTAL_WIDTH, 3), dtype=np.uint8)
        
        # 3. ìˆ˜í™” ì¸ì‹ ë¡œì§
        image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = self.hands.process(image_rgb)

        left_hand, right_hand = None, None
        if result.multi_hand_landmarks:
            for hand_lms, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):
                self.mp_drawing.draw_landmarks(img, hand_lms, self.mp_hands.HAND_CONNECTIONS)
                label = handedness.classification[0].label
                if label == 'Left': left_hand = hand_lms
                else: right_hand = hand_lms
        
        # ë°ì´í„° ìˆ˜ì§‘
        self.seq_buffer.append(extract_xyz(left_hand) + extract_xyz(right_hand))

        # ì˜ˆì¸¡
        if self.model and len(self.seq_buffer) == SEQ_LENGTH:
            input_data = np.array(self.seq_buffer).flatten().reshape(1, -1)
            probs = self.model.predict_proba(input_data)[0]
            idx = np.argmax(probs)
            self.confidence = probs[idx]
            action = self.model.classes_[idx]

            if self.confidence > 0.8:
                self.last_action = action
                if action == 'thankyou':
                    self.tx, self.ty = ROOMS['home'][0], ROOMS['home'][1]
                    self.status = "Returning Home..."
                elif action in ROOMS:
                    self.tx, self.ty = ROOMS[action][0], ROOMS[action][1]
                    self.status = f"Moving to {action.upper()}"

        # 4. ë¡œë´‡ ì—…ë°ì´íŠ¸ ë° ì§€ë„ ê·¸ë¦¬ê¸°
        self.update_robot()
        self.draw_map(canvas)

        # 5. ì˜¤ë¥¸ìª½ í™”ë©´(ì¹´ë©”ë¼) ë°°ì¹˜
        # ì¹´ë©”ë¼ í¬ê¸°ë¥¼ 500x375 ì •ë„ë¡œ ì¡°ì •í•´ì„œ ë°°ì¹˜
        img_resized = cv2.resize(img, (CAM_WIDTH, int(CAM_WIDTH * 0.75)))
        y_offset = (HEIGHT - img_resized.shape[0]) // 2
        canvas[y_offset:y_offset+img_resized.shape[0], MAP_WIDTH:TOTAL_WIDTH] = img_resized

        # 6. ì •ë³´ í…ìŠ¤íŠ¸ (ì˜¤ë¥¸ìª½ ìƒë‹¨)
        info_x = MAP_WIDTH + 20
        cv2.putText(canvas, f"STATUS: {self.status}", (info_x, 50), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(canvas, f"ACTION: {self.last_action.upper()}", (info_x, 90), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        cv2.putText(canvas, f"CONF: {self.confidence*100:.1f}%", (info_x, 130), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        # ìµœì¢… ì˜ìƒì„ Streamlitìœ¼ë¡œ ë°˜í™˜
        return av.VideoFrame.from_ndarray(canvas, format="bgr24")

# =========================================================
# 5. Streamlit UI
# =========================================================
st.set_page_config(page_title="AI Robot Navigation", layout="wide")

st.title("ğŸ¤– Raspbot AI Sign Language Controller")
st.markdown("""
ì™¼ìª½ì€ **ë¡œë´‡ ì‹œë®¬ë ˆì´ì…˜ ë§µ**, ì˜¤ë¥¸ìª½ì€ **ë‚˜ì˜ ì›¹ìº **ì…ë‹ˆë‹¤.  
ìˆ˜í™”ë¥¼ ì¸ì‹í•˜ë©´ ë¡œë´‡ì´ í•´ë‹¹ ì¥ì†Œë¡œ ì´ë™í•©ë‹ˆë‹¤.
""")

# ëª¨ë¸ ë¡œë“œ í™•ì¸
model = load_model()
if model is None:
    st.error("âŒ ëª¨ë¸ íŒŒì¼(model.joblib)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— ë„£ì–´ì£¼ì„¸ìš”.")
else:
    # WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸ ì‹¤í–‰
    ctx = webrtc_streamer(
        key="sign-language",
        video_processor_factory=SignLanguageProcessor,
        rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )
