import streamlit as st
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration
import cv2
import mediapipe as mp
import numpy as np
import joblib
import av
from collections import deque
import math
import os

# =========================================================
# 1. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# =========================================================
# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
MODEL_PATH = 'seq_sign_model_final.joblib' 
SEQ_LENGTH = 40

# ì§€ë„ ë° í™”ë©´ ì„¤ì •
MAP_WIDTH = 500
CAM_WIDTH = 500
HEIGHT = 700
TOTAL_WIDTH = MAP_WIDTH + CAM_WIDTH

# ìƒ‰ìƒ (OpenCVëŠ” BGR ìˆœì„œ)
BG_COLOR = (40, 40, 40)
ROOM_COLOR = (200, 200, 200)
ROBOT_COLOR = (50, 50, 255) # ë¹¨ê°• (BGR)
TEXT_COLOR = (0, 0, 0)

# ë§µ ìœ„ì¹˜
ROOMS = {
    'toilet':   (250, 100, 200, 80),
    'room2':    (250, 220, 200, 80),
    'room1':    (250, 340, 200, 80),
    'elevator': (250, 460, 200, 80),
    'home':     (250, 600, 80, 50)
}

# =========================================================
# 2. ëª¨ë¸ ë¡œë“œ (ìºì‹± ì‚¬ìš©)
# =========================================================
@st.cache_resource
def load_model():
    if not os.path.exists(MODEL_PATH):
        return None
    try:
        return joblib.load(MODEL_PATH)
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# =========================================================
# 3. ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
# =========================================================
def extract_xyz(hand_lms):
    if hand_lms is None: return [0.0] * 63
    out = []
    for lm in hand_lms.landmark:
        out.extend([lm.x, lm.y, lm.z])
    return out

# =========================================================
# 4. ì˜ìƒ ì²˜ë¦¬ê¸° í´ë˜ìŠ¤
# =========================================================
class SignLanguageProcessor(VideoProcessorBase):
    def __init__(self):
        self.model = load_model()
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            max_num_hands=2, 
            min_detection_confidence=0.5, 
            min_tracking_confidence=0.5
        )
        
        self.seq_buffer = deque(maxlen=SEQ_LENGTH)
        
        # ë¡œë´‡ ìƒíƒœ ì´ˆê¸°í™”
        self.rx, self.ry = ROOMS['home'][0], ROOMS['home'][1]
        self.tx, self.ty = self.rx, self.ry
        self.speed = 4
        self.status = "Ready"
        self.last_action = "None"
        self.confidence = 0.0

    def update_robot(self):
        dx = self.tx - self.rx
        dy = self.ty - self.ry
        dist = math.hypot(dx, dy)

        if dist > self.speed:
            self.rx += (dx / dist) * self.speed
            self.ry += (dy / dist) * self.speed
        else:
            self.rx = self.tx
            self.ry = self.ty
            if "Moving" in self.status or "Returning" in self.status:
                self.status = "Arrived"

    def draw_map(self, canvas):
        # ë°°ê²½
        cv2.rectangle(canvas, (0, 0), (MAP_WIDTH, HEIGHT), BG_COLOR, -1)
        # ë³µë„
        cv2.line(canvas, (250, 100), (250, 600), (100, 100, 100), 10)

        # ë°© ê·¸ë¦¬ê¸°
        for name, (cx, cy, w, h) in ROOMS.items():
            color = ROOM_COLOR
            if name == 'home': color = (255, 100, 100) 
            elif name == 'elevator': color = (100, 255, 255) 
            elif name == 'toilet': color = (255, 255, 100) 

            tl = (cx - w//2, cy - h//2)
            br = (cx + w//2, cy + h//2)
            cv2.rectangle(canvas, tl, br, color, -1)
            cv2.rectangle(canvas, tl, br, (255, 255, 255), 2)
            cv2.putText(canvas, name.upper(), (cx - 40, cy + 5), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, TEXT_COLOR, 2)

        # ë¡œë´‡ ê·¸ë¦¬ê¸°
        cv2.circle(canvas, (int(self.rx), int(self.ry)), 15, ROBOT_COLOR, -1)
        cv2.circle(canvas, (int(self.rx), int(self.ry)), 15, (255,255,255), 2)

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        # ìº”ë²„ìŠ¤ ì´ˆê¸°í™”
        canvas = np.zeros((HEIGHT, TOTAL_WIDTH, 3), dtype=np.uint8)
        
        image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = self.hands.process(image_rgb)

        left_hand, right_hand = None, None
        if result.multi_hand_landmarks:
            for hand_lms, handedness in zip(result.multi_hand_landmarks, result.multi_handedness):
                self.mp_drawing.draw_landmarks(img, hand_lms, self.mp_hands.HAND_CONNECTIONS)
                label = handedness.classification[0].label
                if label == 'Left': left_hand = hand_lms
                else: right_hand = hand_lms
        
        # ë°ì´í„° ìˆ˜ì§‘ ë° ì˜ˆì¸¡
        self.seq_buffer.append(extract_xyz(left_hand) + extract_xyz(right_hand))

        if self.model and len(self.seq_buffer) == SEQ_LENGTH:
            input_data = np.array(self.seq_buffer).flatten().reshape(1, -1)
            probs = self.model.predict_proba(input_data)[0]
            idx = np.argmax(probs)
            self.confidence = probs[idx]
            action = self.model.classes_[idx]

            if self.confidence > 0.8:
                self.last_action = action
                if action == 'thankyou':
                    self.tx, self.ty = ROOMS['home'][0], ROOMS['home'][1]
                    self.status = "Returning Home..."
                elif action in ROOMS:
                    self.tx, self.ty = ROOMS[action][0], ROOMS[action][1]
                    self.status = f"Moving to {action.upper()}"

        # ë¡œë´‡ ì—…ë°ì´íŠ¸ ë° ê·¸ë¦¬ê¸°
        self.update_robot()
        self.draw_map(canvas)

        # ì¹´ë©”ë¼ í™”ë©´ ë°°ì¹˜
        img_resized = cv2.resize(img, (CAM_WIDTH, int(CAM_WIDTH * 0.75)))
        y_offset = (HEIGHT - img_resized.shape[0]) // 2
        canvas[y_offset:y_offset+img_resized.shape[0], MAP_WIDTH:TOTAL_WIDTH] = img_resized

        # ì •ë³´ í…ìŠ¤íŠ¸
        info_x = MAP_WIDTH + 20
        cv2.putText(canvas, f"STATUS: {self.status}", (info_x, 50), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(canvas, f"ACTION: {self.last_action.upper()}", (info_x, 90), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        cv2.putText(canvas, f"CONF: {self.confidence*100:.1f}%", (info_x, 130), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        return av.VideoFrame.from_ndarray(canvas, format="bgr24")

# =========================================================
# 5. Streamlit UI
# =========================================================
st.set_page_config(page_title="AI Robot Navigation", layout="wide")

st.title("ğŸ¤– Raspbot AI Sign Language Controller")
st.markdown("""
ì™¼ìª½ì€ **ë¡œë´‡ ì‹œë®¬ë ˆì´ì…˜ ë§µ**, ì˜¤ë¥¸ìª½ì€ **ë‚˜ì˜ ì›¹ìº **ì…ë‹ˆë‹¤.  
ìˆ˜í™”ë¥¼ ì¸ì‹í•˜ë©´ ë¡œë´‡ì´ í•´ë‹¹ ì¥ì†Œë¡œ ì´ë™í•©ë‹ˆë‹¤.
""")

# ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
if not os.path.exists(MODEL_PATH):
    st.warning(f"âš ï¸ `{MODEL_PATH}` íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì—†ìŠµë‹ˆë‹¤. GitHub ë ˆí¬ì§€í† ë¦¬ì— íŒŒì¼ì„ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    # WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸ ì‹¤í–‰
    ctx = webrtc_streamer(
        key="sign-language",
        video_processor_factory=SignLanguageProcessor,
        rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

